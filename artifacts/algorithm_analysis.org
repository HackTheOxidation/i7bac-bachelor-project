* Introduction to the OS2datascanner system
:PROPERTIES:
:CUSTOM_ID: introduction
:END:
This sections provides an introduction to the relevant part of the
OS2datascanner's system architecture along with an explaination of the
goals of doing a Complexity analysis.

** The OS2datascanner System Architecture and Pipeline
:PROPERTIES:
:CUSTOM_ID: the-os2datascanner-system-architecture-and-pipeline
:END:
The OS2datascanner System consists of three larger subsystems: The Admin
system, The Report system and the Engine. We won't discuss the Admin and
Report systems as we are premarily concerned with the rule system in
this analysis, which is part of the Engine. However, do note that the
Admin system and the Report system have their own separate databases,
both of which components of the Engine with interact with.

The engine is structured as a pipeline with a sequence of stages: The
Status Collector stage, the Explorer stage, the Worker stage, the
Exporter stage and the Report Collector stage. Between the stages are
message queues implemented using RabbitMQ, that contain the data that
needs to be processed by a specific stage. All of the stages are
independent in the sense that they are not tied to a specific scanner
job. That is, the system only has one pipeline at all times, that all of
the scanner jobs are processed by. They simply proces any available
incoming work from a relevant message queue.

The engine pipeline architecture as described by the official
[[https://os2datascanner.readthedocs.io/architecture/pipeline-architecture.svg][OS2datascanner Documentation on ReadTheDocs.io]] can be seen on figure [[pipeline]].

#+caption: OS2datascanner Pipeline Architecture. Generously provided by the OS2datascanner team.
#+name: pipeline
[[./pipeline-architecture.png]]

*** The Status Collector stage
:PROPERTIES:
:CUSTOM_ID: the-status-collector-stage
:END:
The Status Collector stage is responsible for keeping the Admin systems
database updated with the status' of the scanner jobs, which is done by
reading messages from the "status" queue.

*** The Explorer stage
:PROPERTIES:
:CUSTOM_ID: the-explorer-stage
:END:
The Explorer stage finds all objects that need to be scanned by the
system for a given data source. It starts by reading a message from the
"scan spec" queue, which contains a specification for a scanner job.
These messages come from the admin system and include information about
the data source as well as what rules the scanner should use. Depending
on the type of the scanner job and thus the type of the data source, it
performs various actions to find all of the scannable objects in that
data source and produce a message for the "conversions" queue for each
object found. Note, that it does not traverse the data source
recursively, so some of the objects may still contain other nested
objects.

*** The Worker stage
:PROPERTIES:
:CUSTOM_ID: the-worker-stage
:END:
Once the scannable objects have been found, the Worker stage starts
converting the object into scannable text. This is handled differently
depending on the type of object. This part of the Worker stage is known
as the Processor stage.

If the Processor determines that an object contains nested objects that
need to be explored further, the Worker stage has its own internal
Explorer stage for handling these objects. On the other hand, if the
conversion done by the Processor produces scannable text, the Matcher
tries to match the converted text against one or more specified rules.
The Matcher might find data in the converted object that it cannot scan
and may send it back to the Processor for further conversion.

If the Matcher finds a piece of text that matches a rule, it sends the
match to the Tagger stage which format and adds additional metadata
about the match and sends it to the "match" and "metadata" queues. While
performing all of these mentioned operations, the Worker stage also
produce messages for some other queue. It sends updates about the
scanner job to the "status" queue. In case that a error occures, the
Worker stage sends a message to the "problem" queue to notify the rest
of the system.

*** The Exporter stage
:PROPERTIES:
:CUSTOM_ID: the-exporter-stage
:END:
The Exporter stage aggregates matches and metadata from the "match" and
"metadata" queues respectively. Then it combines the related matches and
metadata and formats it to messages that are sent to the "results" queue
to be received by the report collector.

*** The Report Collector stage
:PROPERTIES:
:CUSTOM_ID: the-report-collector-stage
:END:
Finally the Report Collector stage reads messages from the "result"
queue produced by the Exporter stage and updates the Report systems
database accordingly.

*** Relevant Engine components
:PROPERTIES:
:CUSTOM_ID: relevant-engine-components
:END:
The stage of interest for this analysis is the "Matcher" stage, which
uses rules to match against some textual content. A rule is anything
that implements a =match()=-method which takes a string and yields a
python iterator. The system comes with some specialized rules: The
=CPRRule= is designed to detect danish CPR-numbers. The =NameRule= and
=AddressRule= find names and addresses, respectively. It is the
underlying algorithms for the =match()=-method implemenation of each of
these rules that will be analyzed.

** The Goals of this Complexity Analysis
:PROPERTIES:
:CUSTOM_ID: the-goals-of-this-complexity-analysis
:END:
Along with benchmarks, complexity analysis of the rule system will serve
as a theoritical basis for evaluating the efficiency of each of the
underlying algorithms and compare new proposals for replacements of the
rule system.

The goal of the analysis is to obtain an Asymptotic Upper-bound,
\(\mathcal{O}\), for all the rules. This gives us a way to compare the
efficiency of new implementations in a platform/hardware-independent
manor.

** A comment about the code
:PROPERTIES:
:CUSTOM_ID: a-comment-about-the-code
:END:
The code in the current version of the OS2datascanner system is written
in =python= specifically for version =3.9= and earlier, where =match=
was not considered a reserved keyword. Some of the lengthy comments and
docstrings have also been removed to draw attention to the functioning
code.

* The Complexity Analysis of the Rule system
:PROPERTIES:
:CUSTOM_ID: the-analysis
:END:
In this section, the source code for some of the rules in the rule
system of OS2datascanner are analyzed in order to obtain an Asymptotic
upper-bound \(\mathcal{O}\) for each of the underlying algorithms. The
source code for the OS2datascanner system is available on the projects
[[https://github.com/os2datascanner/os2datascanner][GitHub page]]. The
code is not written by the author of this paper and has not been
modified from the original.

** CPR Rule
:PROPERTIES:
:CUSTOM_ID: cpr-rule
:END:
The CPR Rule comes with three options, which changes the control flow
and thus the outcome of the underlying algorithm. Therefore we will need
to divide the analysis of the CPR Rule algorithm into four cases
analysing the default algorithm (no options enabled) and the algorithms
associated with each of the options: =check_mod11=, =ignore_irrelevant=
and =examine_context=.

But first, we provide a higher-level, conceptual overview of how the
algorithm works. On figure [[Figur 2]], the activity diagram shows the overall
flow of the algorithm behind the =CPRRule=.

#+caption: Activity diagram for the CPRRule algorithm
#+NAME: Figur 2
#+ATTR_LATEX: :width 5cm :height 18cm
[[file:cpr_activity.png]]

First, the content text is searched for substrings of ten digits that
matches a specific regex. Then, depending on the options enabled, one or
more of the following may occur:

1. =check_mod11= enabled: Calculate Modulus 11 sum and discard strings
   with non-zero sums.
2. =ignore_irrelevant= enabled: Check that the first six digits
   constitute a valid date.
3. =examine_context= enabled: Check the surrounding text for blacklisted
   words.

Finally a match result is yielded.

*** The default algorithm (no options enabled)
:PROPERTIES:
:CUSTOM_ID: the-default-algorithm-no-options-enabled
:END:
We seek to model the time-complexity of the matching-algorithm as a
multivariate function \(T(n, m)\), where \(n\) is the length of the
input string and \(m\) is the number of matches in the input string.

For the sake of simplicity, we will omit the cost of the logging
statements in the complete code.

The source code of interest is the =match()= function of the =CPRRule=
class:

#+begin_src python
    def match(self, content: str) -> Optional[Iterator[dict]]:
        if content is None:
            return

        if self._examine_context and self._blacklist:
            if (m := self._blacklist_pattern.search(content.lower())):
                logger.debug("Blacklist matched content", matches=m.group(0))
                return

        imatch = 0
        for itot, m in enumerate(self._compiled_expression.finditer(content), 1):
            cpr = m.group(1).replace(" ", "") + m.group(2)
            if self._modulus_11:
                mod11, reason = modulus11_check(cpr)
                if not mod11:
                    logger.debug(f"{cpr} failed modulus11 check due to {reason}")
                    continue

            probability = 1.0
            if self._ignore_irrelevant:
                probability = calculator.cpr_check(cpr, do_mod11_check=False)
                if isinstance(probability, str):
                    logger.debug(f"{cpr} is not valid cpr due to {probability}")
                    continue

            cpr = cpr[0:4] + "XXXXXX"
            low, high = m.span()
            # only examine context if there is any
            if self._examine_context and len(content) > (high - low):
                p, ctype = self.examine_context(m)
                # determine if probability stems from context or calculator
                probability = p if p is not None else probability
                ctype = ctype if ctype != [] else Context.PROBABILITY_CALC
                logger.debug(f"{cpr} with probability {probability} from context "
                             f"due to {ctype}")

            if probability:
                imatch += 1
                yield {
                    "match": cpr,

                    **make_context(m, content),

                    "sensitivity": (
                        self.sensitivity.value
                        if self.sensitivity
                        else self.sensitivity
                    ),
                    "probability": probability,
                }
            logger.debug(f"{itot} cpr-like numbers, "
                         f"of which {imatch} had a probabiliy > 0")
#+end_src

The first line in the function is a comparison to check that the input
=content= is not =None= (non-empty/non-null), which will incur a
constant-time computation of \(T_{1}\). The next =if=-statement checks
if the "Examine Context" option is enabled and that the collection of
blacklisted words are not =None=. This is also a constant-time
computation of \(T_{2}\). Since "Examine Context" is not enabled in this
case, the inner block of the =if=-statement is skipped. After that, an
assignment operation =imatch = 0= occures in constant-time: \(T_3\).
Next, a =for=-loop is reached, where the =context= string is searched
for occurances of any substring that matches the regular expression:

#+begin_src python
    cpr_regex = r"\b(\d{2}[ ]?\d{2}[ ]?\d{2})(?:[ \-/\.\t]|[ ]\-[ ])?(\d{4})\b"
#+end_src

This is done using the =finditer()=-method from the
[[https://docs.python.org/3/library/re.html#re.finditer][=re=]] module
in python's standard library, which runs in linear time proportional to
the length of the input string. That is, it has a time cost of
\(T_3\cdot n\). Then, for each match, \(m\), a variable =cpr= is
assigned to the match where whitespace has been removed. This happens as
a constant-time computation: \(T_4\), since the maximum length of a
match is bound by the =cpr_regex=. In the next =if=-statement, it is
checked whether the "Modulus-11" option is enabled in constant-time,
\(T_5\). After that, the variable =probability= is initialized, in
constant-time: \(T_6\). Subsequently, it is checked whether the "Ignore
Irrelevant" option is enabled, in constant-time: \(T_7\). Next, the
=cpr= variable is reassigned, in constant-time: \(T_8\), and also the
=low= and =high= variables are initialized, in constant-time: \(T_9\).
Again, the subsequent =if=-statement checks if the "Examine Context"
option is enabled and if the length of the =content= string is longer
than the value of =high= minus =low=. Since the "Examine Context" is not
enabled and thus false in this case, short-circuiting is triggered and
the computation runs in constant-time: \(T_{10}\). Then, it is check
whether the =probability= variable is initialized or not =None=, which
it isn't in this case, in constant-time: \(T_{11}\). After that, the
=imatch= variable is incremented, in constant time \(T_{12}\). Finally,
the =yield= statement returns a python =dict= (map) with data of
interest about a match as an iterator. Of particular interest is the
call the the =make\_context()= function, whose complexity can be
obtained by inspecting the source code:

#+begin_src python
    _context_filters = []
    
    
    def add_context_filter(func):
        """Registers a filter function for contexts. All contexts returned by the
        make_context function will be passed through this function first.
    
        Context filters are called in the order in which they're added."""
        _context_filters.append(func)
    
    
    def make_context(match, text, func=None):
        """Returns the (optionally postprocessed) context surrounding a match."""
        if isinstance(match, tuple):
            low, high = match
        else:
            low, high = match.span()
        ctx_low, ctx_high = max(low - 50, 0), high + 50
        # Extract context, remove newlines and tabs for better representation
        match_context = " ".join(text[ctx_low:ctx_high].split())
    
        for f in _context_filters + ([func] if func else []):
            match_context = f(match_context)
    
        return {
            "offset": low,
            "context": match_context,
            "context_offset": low - ctx_low
        }
#+end_src

This function runs in constant-time: \(T_{13}\), due to a couple of
assumptions: firstly, we know that that length of a =match= is at most
13 characters, and secondly, that the computations that involve the
=text= string do not depend on its length. Thirdly, in this case the
=func= argument is =None= and there is only one element in
=_context_filters=, which is:

#+begin_src python
    # Attempt to filter CPR number-like strings out of all contexts
    add_context_filter(
        partial(
            re.compile(cpr_regex).sub,
            "XXXXXX-XXXX"))
#+end_src

since the maximum length of =match_context= is at most 113 characters
this is also a constant-time computation.

To summarize, we now have the function, \(T(n, m)\):

\begin{equation*}
T(n, m) = T_1 + T_2 + T_3\cdot n + m(T_4 + T_5 + T_6 + T_7 + T_8 + T_9 + T_{10} + T_{11} + T_{12} + T_{13})
\end{equation*}

Suppose there exists \(k \geq max(\{T_1,...,T_{13}\})\), then

\begin{align*}
k + k + kn + &m(k + k + k + k + k + k + k + k + k) \geq \\
 T_1 + T_2 + T_3\cdot n + &m(T_4 + T_5 + T_6 + T_7 + T_8 + T_9 + T_{10} + T_{11} + T_{12} + T_{13}) \\
&\Downarrow \\
2k + kn + 9k&m \geq \\
T_1 + T_2 + T_3\cdot n + &m(T_4 + T_5 + T_6 + T_7 + T_8 + T_9 + T_{10} + T_{11} + T_{12} + T_{13}) \\
&\Downarrow \\
T(n, m) = 2k + kn + 9k&m = \mathcal{O}(n + m)
\end{align*}

And thus we have proved that the Asymptotic Upper-bound for the default
version of the matching algorithm of the =CPRRule= is
\(\mathcal{O}(n + m)\).

*** The Modulus-11 checking option
:PROPERTIES:
:CUSTOM_ID: the-modulus-11-checking-option
:END:
According to the specification for the CPR-number system, CPR-numbers
prior to January 1st 2007 had to fulfill a mathematical property called
"The Modulus 11 Rule". Briefly described, each digit in a CPR-number is
multiplied by special factor and the sum of this has to be divisible
by 11. There are a few exceptions to this, which is listed in the
specification. As seen in the analysis of the default algorithm, if the
=modulus_11=-option is enabled, a sub-algorithm is run in the =for=-loop
of the =match()=-method:

#+begin_src python
    for itot, m in enumerate(self._compiled_expression.finditer(content), 1):
        # ...
        if self._modulus11:
            mod11, reason = modulus11_check(cpr)
            if not mod11:
                continue
#+end_src

Of most importance is the =modulus11_check(cpr)=-function call, which
has the following definition:

#+begin_src python
def modulus11_check(cpr: str) -> Tuple[bool, str]:
    try:
        birth_date = get_birth_date(cpr)
        # IndexError if cpr is less than 7 chars
    except (ValueError, IndexError):
        return False, "malformed birth_date"

    # Return True if the birth dates are one of the exceptions to the
    # modulus 11 rule.
    if birth_date in CPR_EXCEPTION_DATES:
        return True, "in exception_date"
    else:
        # Otherwise, perform the modulus-11 check
        return modulus11_check_raw(cpr), "due to modulus11"
#+end_src

First, the birth date must be extracted using the =get_birth_date()=
function. Due to the format of a CPR-number, the fifth and sixth digits,
which represents the year a person was born, do not contain any
information about the century/millenia. This is indicated by the seventh
digit in combination with the fifth and sixth digits.

#+begin_src python
def get_birth_date(cpr: str) -> date:
    """Get the birth date as a datetime from the CPR number.

    If the CPR has an invalid birthday, raises ValueError.
    """
    day = int(cpr[0:2])
    month = int(cpr[2:4])
    year = int(cpr[4:6])

    year_check = int(cpr[6])

    # Convert 2-digit year to 4-digit:
    if year_check >= 0 and year_check <= 3:  # in (0,1,2,3)
        year += 1900
    elif year_check == 4:
        if year > 36:
            year += 1900
        else:
            year += 2000
    elif year_check >= 5 and year_check <= 8:  # in (5,6,7,8)
        if year > 57:
            year += 1800
        else:
            year += 2000
    elif year_check == 9:
        if year > 37:
            year += 1900
        else:
            year += 2000

    return date(day=day, month=month, year=year)
#+end_src

Although there are many conditions to check, the
=get_birth_date()=-function runs in constant time, since the length of a
CPR-number is always the same and due to the fact that the function does
not contain any looping or iterating logic.

Supposing that the =birth_date= is succesfully retrieved, it is then
check whether the birth date is one of the few exception dates before
January 1st 2007 that are allowed to fail the modulus 11 rule. If not, a
call to the =modulus11_check_raw()= function is made, which performs the
actual calculation of the modulus 11 remainder and checks that it is 0:

#+begin_src python
def modulus11_check_raw(cpr: str) -> bool:
    """Check if the CPR fulfils the modulus-11 check

    This should not be called directly as it does not make any exceptions
    for numbers for which the modulus-11 check should not be performed.
    """
    return sum([int(c) * v for c, v in zip(cpr, _mod_11_table)]) % 11 == 0
#+end_src

This also runs in constant time.

All in all, the modulus 11 check is run for every occurance of a
potential match in the =content= string. That is, \(m\) times. And since
every part of this subroutine runs in constant time, the overall time
complexity of the Asymptotic Upper-bound of the algorithm behind the
=match()=-method remains the same: \(\mathcal{O}(n + m)\).

*** The "Ignore Irrelevant" option
:PROPERTIES:
:CUSTOM_ID: the-ignore-irrelevant-option
:END:
As seen previously in the analysis of the default algorithm, if the
=ignore_irrelevant=-option is enabled, a sub-algorithm is run in the
=for=-loop of =match()=-method:

#+begin_src python
    for itot, m in enumerate(self._compiled_expression.finditer(content), 1):
        # ...
        if self._ignore_irrelevant:
            probability = calculator.cpr_check(cpr, do_mod11_check=False)
            if isinstance(probability, str):
                continue
#+end_src

Of particular interest is the =calculator.cpr_check()=-function:

#+begin_src python
    def cpr_check(self, cpr: str, do_mod11_check=True) -> Union[str, float]:
        error = self._form_validator(cpr)
        if error:
            return error

        birth_date = get_birth_date(cpr)
        if birth_date > date.today():
            return "CPR newer than today"

        # we cannot say anything about the probability, when the date is an
        # exception-date
        if birth_date in CPR_EXCEPTION_DATES:
            return 0.5

        if (do_mod11_check and not modulus11_check_raw(cpr) and
                birth_date not in CPR_EXCEPTION_DATES):
            return "Modulus 11 does not match"

        legal_cprs = self._calc_all_cprs(birth_date)
        try:
            index_number = legal_cprs.index(cpr)
        except ValueError:
            return "CPR is not a legal value"

        if index_number <= 100:
            return 1.0
        elif 100 < index_number <= 200:
            return 0.8
        elif 200 < index_number <= 250:
            return 0.6
        elif 250 < index_number <= 350:
            return 0.25
        else:
            return 0.1
#+end_src

A few interesting computations happen here: first, the
=_form_validator()= is called:

#+begin_src python
    @staticmethod
    def _form_validator(cpr: str) -> str:
        if len(cpr) < 10:
            return "CPR too short"
        if len(cpr) > 10:
            return "CPR too long"
        if not cpr.isdigit():
            return "CPR can only contain digits"

        try:
            get_birth_date(cpr)
        except ValueError:
            return "Illegal date"
        return ""
#+end_src

It simply checks that the length of the potential CPR-number is exactly
ten characters and that it only contains digits, which runs in constant
time (the =get_birth_date()=-function has been analyzed in the previous
section).

Then, the birth date is extracted using =get_birth_date()=, it is
checked that the date is not in the future and that it is not one of the
exception dates.

Subsequently, it is checked whether the "Modulus 11" options is enabled.
To keep things separated, for the purpose of this analysis, we assume
that it isn't enabled.

After that, all possible CPR-numbers for that birth date is calculated
by calling the =self._calc_all_cprs(birth_date)= with =birth_date= as an
argument:

#+begin_src python
    def _calc_all_cprs(self, birth_date: date) -> list:
        """Calculate all valid CPRs for a given birth date.

        :param birh_date: The birh date to check.
        :return: A list of all legal CPRs for that date.
        """
        cache_key = str(birth_date)
        if cache_key in self.cached_cprs:
            return self.cached_cprs[cache_key]
        legal_7 = self._legal_7s(birth_date.year)

        legal_cprs = []
        for index_7 in legal_7:
            for i in range(0, 1000):
                cpr_candidate = (
                    birth_date.strftime("%d%m%y")
                    + str(index_7)
                    + str(i).zfill(3)
                )
                valid = modulus11_check_raw(cpr_candidate)
                if valid:
                    legal_cprs.append(cpr_candidate)

        self.cached_cprs[cache_key] = legal_cprs
        return legal_cprs
#+end_src

We won't go into too much detail about this method, but it runs in
constant time, because the number of legal CPR-numbers for any birth
date is a constant and does not depend on the input size.

Back to the =cpr_check()=-method. The last thing that happens is
determining the =index_number= for the particular birth date, which in
turn determines the probability of the suspected match actually being a
real CPR-number. That is, unless it is actually an invalid date. \

To summarize, enabling the =ignore_irrelevant=-option does not change
the time complexity class for the algorithm of \(\mathcal{O}(n + m)\),
though it may alter the constant factors drastically.

*** The "Examine Context" option
:PROPERTIES:
:CUSTOM_ID: the-examine-context-option
:END:
Not every 10-digit number satisfies all the criteria for being a valid
CPR-number. It may simply be a coincidence. It could be something else
entirely, such as a product number, an order number or the like. Other
times a number with an odd format might actually be representing a
CPR-number. In other to reduce the risk of reporting a lot of false
positive matches, the =examine_context=-option checks the text
surrounding potential match to search for occurances of words that are
either =blacklisted= or =whitelisted=.

Enabling the =examine_context=-option has implications in two places in
the =match()=-method: 1) In the beginning:

#+begin_src python
    if self._examine_context and self._blacklist:
        if (m := self._blacklist_pattern.search(content.lower())):
            return
#+end_src

And 2) in the =for=-loop:

#+begin_src python
    for itot, m in enumerate(self._compiled_expression.finditer(content), 1):
        # only examine context if there is any
        if self._examine_context and len(content) > (high - low):
            p, ctype = self.examine_context(m)
            # determine if probability stems from context or calculator
            probability = p if p is not None else probability
            ctype = ctype if ctype != [] else Context.PROBABILITY_CALC
#+end_src

In 1) the the =content= string is searched for occurances of substrings
that are in the blacklist, provided that the blacklist is not =None=.
First, all characters in the =content= are changed to lower-case with
=.lower()=-method, which is a linear-time computation as it depends on
the length of the =content= string, \(n\). Subsequently, the lower-cased
text is searched using the =search()=-method on the
=_blacklist_pattern=, which is a compiled regular expression (using
python's =re=-module). The default =_blacklist_pattern= uses the
=BLACKLIST_WORDS= dict:

#+begin_src python
    BLACKLIST_WORDS = {
        "p-nr", r"p\.nr", "p-nummer", "pnr",
        "customer no", "customer-no",
        "bilagsnummer",
        "order number", "ordrenummer",
        "fakturanummer", "faknr", "fak-nr",
        "tullstatistisk", "tullstatistik",
        "test report no",
        r"protocol no\.",
        "dhk:tx",
    }
#+end_src

wherein all the strings are combined to a single compiled regular
expression using the =|=-operator in the =CPRRule= constructor:

#+begin_src python
    ...
    self._blacklist = self.BLACKLIST_WORDS if blacklist is None else set(blacklist)
    self._blacklist_pattern = re.compile("|".join(self._blacklist))
    ...
#+end_src

The =search()=-function takes linear time to complete, as it is
dependent on the length of the =content= string, \(n\).

In 2) the length of the =content= string is asserted to be longer than
the length of the potential match, such that there is actually some
context to examine. Next, the =examine_context()=-method is called on
the =CPRRule= class instance, which is the interesting part. The
=examine_context()=-methods has the following definition:

#+begin_src python
    def examine_context(  # noqa: CCR001, C901 too high cognitive complexity
        self, match: Match[str]
    ) -> Tuple[Optional[float], List[tuple]]:

        probability = None
        words, symbols = self.extract_surrounding_words(match, n_words=3)
        ctype = []

        # test if a whitelist-word is found in the context words.
        # combine the list of 'pre' & 'post' keys in words dict.
        words_lower = [w.lower() for w in chain.from_iterable(words.values())]
        if self._whitelist:
            for w in self._whitelist:
                for cw in words_lower:
                    if w in cw:
                        ctype.append((Context.WHITELIST, cw))
                        return 1.0, ctype

        # test for balanced delimiters
        delimiters = 0
        for w in chain.from_iterable(symbols.values()):
            if w.startswith(_pre_delim):
                delimiters += 1
            elif w.endswith(_pre_delim):
                delimiters += 1
            elif w.startswith(_post_delim):
                delimiters -= 1
            elif w.endswith(_post_delim):
                delimiters -= 1
            elif w in _all_symbols:
                ctype.append((Context.SYMBOL, w))
                probability = 0.0
        if delimiters != 0:
            ctype.append((Context.UNBALANCED, delimiters))
            probability = 0.0

        # only do context checking on surrounding words
        for w in [words["pre"][-1], words["post"][0]]:
            if w == "" or self._compiled_expression.match(w):
                continue
            # this check is newer reached due to '\w' splitting
            elif w.endswith(_all_symbols) or w.startswith(_all_symbols):
                ctype.append((Context.SYMBOL, w))
                probability = 0.0
            # test if surrounding word is a number (and not looks like a cpr)
            elif is_number(w):
                probability = 0.0
                ctype.append((Context.NUMBER, w))
            elif not is_alpha_case(w):
                # test for case, ie Magenta, magenta, MAGENTA are ok, but not MaGenTa
                # nor magenta10. w must not be empty string
                probability = 0.0
                ctype.append((Context.WRONG_CASE, w))
            else:
                pass

        return probability, ctype
#+end_src

First, a call to the method
=self.extract_surrounding_words(match, n_words=3)= is made to inspect
the characters surrounding a match for occurances of words and symbols
of interest. The =extract_surrounding_words()=-method is defined as:

#+begin_src python
    def extract_surrounding_words(
        self, match: Match[str], n_words: int = 2
    ) -> Tuple[Dict[str, list], Dict[str, list]]:
        """Extract at most `n_words` before and after the match

        Return a dict with words and one with symbols
        """

        # get full content
        content = match.string
        low, high = match.span()
        # get previous/next n words
        pre = " ".join(content[max(low-50, 0):low].split()[-n_words:])
        post = " ".join(content[high:high+50].split()[:n_words])

        # split in two capture groups: (word, symbol)
        # Ex: 'The brown, fox' ->
        # [('The', ''), ('brown', ''), ('', ','), ('fox', '')]
        word_str = r"(\w+(?:[-\./]\w*)*)"
        symbol_str = r"([^\w\s\.\"])"
        split_str = r"|".join([word_str, symbol_str])
        pre_res = re.findall(split_str, pre)
        post_res = re.findall(split_str, post)
        # remove empty strings
        pre_words = [s[0] for s in pre_res if s[0]]
        post_words = [s[0] for s in post_res if s[0]]
        pre_sym = [s[1] for s in pre_res if s[1]]
        post_sym = [s[1] for s in post_res if s[1]]

        # XXX Should be set instead?
        words = dict(
            pre=pre_words if len(pre_words) > 0 else [""],
            post=post_words if len(post_words) > 0 else [""],
        )
        symbols = dict(
            pre=pre_sym if len(pre_sym) > 0 else [""],
            post=post_sym if len(post_sym) > 0 else [""],
        )
        return words, symbols
#+end_src

This is a rather lengthy method. We won't go deeper into the details of
the analysis of this method, but it has a constant running time, since
none of the computations depend on the length of the =content= (input)
string, although a lot of computations take place.

Next, each of the extracted words and symbols surrounding the suspected
match are checked in various ways. First, it is checked to see if one of
them is a word in the =whitelist= in which it is immediately deemed to
be a certain match. After that, it is checked that all delimiters in the
surrounding symbols are balanced. If not, the probability of a match
decreases to 0. Finally, it is checked whether the surrounding words are
numbers, symbols (like delimiters) or if a word has improper casing, in
which case the probability is reduced to 0 and the match is rejected. \

All in all, enabling the =examine_context=-option doesn't change the
time complexity class of the algorithm which is still
\(\mathcal{O}(n + m)\).

** Name Rule
:PROPERTIES:
:CUSTOM_ID: name-rule
:END:
The Name Rule is designed to detect textual occurances of names, whether
it is a firstname, lastname or both. It doesn't necessarily have to be
the name of a person, it could be the name of many other things such as
cities or countries.

On figure [[Figur 3]], the activity diagram shows the overall flow of the
algorithm behind the =NameRule=:

#+caption: Activity diagram for NameRule algorithm
#+NAME: Figur 3
#+ATTR_LATEX: :width 7cm :height 18cm
[[file:name_activity.png]]

Speaking of code, all of this is encapsulated by the =NameRule= class in
the source code. It's =match()= method implements a name detection
algorithm:

#+begin_src python

    def match(self, text):  # noqa: CCR001, too high cognitive complexity
        self._load_datasets()
        unmatched_text = text

        def is_name_component(
                component: str,
                *candidate_sets: set[str]):
            component = component.upper()
            if component in self._blacklist:
                return True
            elif component in self._whitelist:
                return False
            else:
                return any(component in cs for cs in candidate_sets)

        # First, check for whole names, i.e. at least Firstname + Lastname
        names = match_full_name(text)
        for match, first_name, middle_names, last_name, matched_text in names:
            middle_names = list(middle_names)
            last_name = last_name or ""

            # Match each name against the list of first and last names
            first_match = is_name_component(first_name, self.first_names)
            last_match = is_name_component(last_name, self.last_names)
            middle_match = any(
                is_name_component(n, self.first_names, self.last_names)
                for n in middle_names
            )
            # But what if the name is Word Firstname Lastname?
            while middle_match and not first_match:
                old_name = first_name
                first_name = middle_names.pop(0)
                first_match = is_name_component(first_name, self.first_names)
                middle_match = any(
                    is_name_component(n, self.first_names, self.last_names)
                    for n in middle_names
                )
                matched_text = matched_text.lstrip(old_name)
                matched_text = matched_text.lstrip()
            # Or Firstname Lastname Word?
            while middle_match and not last_match:
                old_name = last_name
                last_name = middle_names.pop()
                last_match = is_name_component(last_name, self.last_names)
                middle_match = any(
                    is_name_component(n, self.first_names, self.last_names)
                    for n in middle_names
                )
                matched_text = matched_text.rstrip(old_name)
                matched_text = matched_text.rstrip()

            if middle_names:
                full_name = "%s %s %s" % (
                    first_name, " ".join(middle_names), last_name
                )
            else:
                full_name = "%s %s" % (first_name, last_name)

            full_name_up = full_name.upper()
            # Check if name is blacklisted.
            # The name is blacklisted if there exists a string in the
            # blacklist which is contained as a substring of the name.
            is_blacklisted = any(b in full_name_up for b in self._blacklist)
            # Name match is always high probability
            # and occurs only when first and last name are in the name lists
            # Set probability according to how many of the names were found
            # in the names lists
            if (first_match and last_match) or is_blacklisted:
                probability = 1.0
            elif first_match or last_match or middle_match:
                probability = 0.5
            else:
                continue

            # If we have to do a second pass, cut this matched name out to
            # avoid duplicates
            if self._expansive:
                unmatched_text = unmatched_text.replace(matched_text, "", 1)

            yield {
                "match": matched_text,
                "probability": probability,

                **make_context(match, text),

                "sensitivity": (
                    self.sensitivity.value if self.sensitivity else None
                ),
            }
        if self._expansive:
            # Full name match done. Now check if there's any standalone names
            # in the remaining, i.e. so far unmatched string.
            name_regex = regex.compile(_name)
            it = name_regex.finditer(unmatched_text, overlapped=False)
            for m in it:
                matched = m.group(0)
                if is_name_component(
                        matched.upper(), self.first_names, self.last_names):
                    yield {
                        "match": matched,
                        "probability": 0.1,

                        # XXX: are the offsets here useful? (unmatched_text is
                        # something we've produced internally...)
                        **make_context(m, unmatched_text),

                        "sensitivity": (
                            self.sensitivity.value
                            if self.sensitivity else None
                        ),
                    }
#+end_src

We will divide this analysis into two: the default version and the
version having the =expansive= option enabled to break down the
complexity.

*** The default algorithm
:PROPERTIES:
:CUSTOM_ID: the-default-algorithm
:END:
The first thing that takes place in the =match()= method is a call to
the =self._load_datasets()= method. This method will load and cache some
datasets of officially recognized Danish names into instance variables
=self.last_names= and =self.first_names=:

#+begin_src python
    def _load_datasets(self):
        if self.first_names is None:
            # Convert list of str to upper case and to sets for efficient
            # lookup
            m = set(map(str.upper,
                    common_loader.load_dataset(
                            "names", "da_20140101_dst_fornavne-mænd")))
            k = set(map(str.upper,
                    common_loader.load_dataset(
                            "names", "da_20140101_dst_fornavne-kvinder")))
            e = set(map(str.upper,
                    common_loader.load_dataset(
                            "names", "da_20140101_dst_efternavne")))
            f = m.union(k)

            self.last_names = e
            self.first_names = f
#+end_src

These computations are heavy, but still run in constant time, \(T_1\),
under the assumption that the data sets won't change and the fact that
this is only run the first time the =_load_datasets()=-method is called.

Next, a locally-scoped function =is_name_component()= is declared for
later use:

#+begin_src python
        def is_name_component(
                component: str,
                *candidate_sets: set[str]):
            component = component.upper()
            if component in self._blacklist:
                return True
            elif component in self._whitelist:
                return False
            else:
                return any(component in cs for cs in candidate_sets)
#+end_src

The running time of this function is linear, but depends on the length
of the =component= and the number of elements in both =candidate_sets=,
=self._blacklist= and =self._whitelist=, which could vary greatly.

The next interesting thing is the call to the
=match_full_name()=-method:

#+begin_src python
    # First, check for whole names, i.e. at least Firstname + Lastname
    names = match_full_name(text)
#+end_src

The =match_full_name()=-function has the following definition:

#+begin_src python
def match_full_name(text):
    """Return possible name matches in the given text as a `set`."""

    def strip_or_empty(string):
        return string.strip() if string else ""

    matches = set()
    it = full_name_regex.finditer(text, overlapped=False)
    for m in it:
        first = strip_or_empty(m.group("first"))
        middle = strip_or_empty(m.group("middle"))
        if middle:
            middle_split = tuple(
                regex.split(r'\s+', middle.lstrip(), regex.UNICODE))
        else:
            middle_split = ()
        last = strip_or_empty(m.group("last"))
        matched_text = m.group(0)
        matches.add((m, first, middle_split, last, matched_text))
    return matches
#+end_src

It finds occurances of full names by using a regular expression
=full_name_regex=:

#+begin_src python
_whitespace = (
        r"[^\S\n\r]+"  # One or more of every whitespace character (apart from
                       # new lines)
)

_simple_name = (
        r"\p{Lu}"          # One upper-case letter...
        r"(?:\p{L}+|\.?)"  # followed by one or more letters, a full stop, or
                           # nothing
)
# (for example, "Joe", "Bloggs", "Bulwer", "K.", "J", or "Edward")

_name = (
        rf"{_simple_name}"        # A simple_name...
        rf"(?:-{_simple_name})?"  # optionally hyphenated with another one
)
# (for example, "Jens", "Bulwer-Lytton", "You", "United", or "B.-L.")

full_name_regex = regex.compile(  # noqa: ECE001, expression is too complex
    rf"\b(?P<first>{_name})"               # A name at the start of a word...
    rf"(?P<middle>({_whitespace}{_name})"
    r"{0,3})"                              # followed by zero to three more
                                           # whitespace-separated names...
    rf"(?P<last>{_whitespace}{_name})\b"   # followed by another name and the
                                           # end of the word
)
# (for example, "Joe Bloggs", "Josef K.", "L. Frank Baum", "Edward George Earle
# Lytton Bulwer-Lytton", "Jens J-J. Jens-Jens Jens Jensen", "United Kingdom",
# or "You Are A Winner")
#+end_src

Note that the =full_name_regex= regular expression allows up to 5
=_name=s to appear in sequence. Calling the =find_iter()=-function with
=full_name_regex= results in a computation that runs in linear time,
\(T_2\cdot n\). Then, for each occurance of a 'full name', it is added
to a =set()= and returned. The running time of that operation is linear
depending on the number of occurances found, \(m\).

Returning to the =match()=-function, for each 'full name' found using
=match_full_name()=, a number of checks are made to ensure that the
presumed matches are actual names as determined by the
=self.first_names= and =self.last_names= datasets loaded previously.
This is done using the locally-scoped =is_name_component()=-function
declared earlier:

#+begin_src python
            middle_names = list(middle_names)
            last_name = last_name or ""

            # Match each name against the list of first and last names
            first_match = is_name_component(first_name, self.first_names)
            last_match = is_name_component(last_name, self.last_names)
            middle_match = any(
                is_name_component(n, self.first_names, self.last_names)
                for n in middle_names
            )
#+end_src

The running time of these computations depend on the number of element
in the datasets =self.first_names= and =self.last_names=, however
assuming that these are constant, it can be done in constant time,
\(T_3\).

There are a few edge cases that need to be checked.

#+begin_src python
            # But what if the name is Word Firstname Lastname?
            while middle_match and not first_match:
                old_name = first_name
                first_name = middle_names.pop(0)
                first_match = is_name_component(first_name, self.first_names)
                middle_match = any(
                    is_name_component(n, self.first_names, self.last_names)
                    for n in middle_names
                )
                matched_text = matched_text.lstrip(old_name)
                matched_text = matched_text.lstrip()
            # Or Firstname Lastname Word?
            while middle_match and not last_match:
                old_name = last_name
                last_name = middle_names.pop()
                last_match = is_name_component(last_name, self.last_names)
                middle_match = any(
                    is_name_component(n, self.first_names, self.last_names)
                    for n in middle_names
                )
                matched_text = matched_text.rstrip(old_name)
                matched_text = matched_text.rstrip()
#+end_src

This covers cases where, for example, there is a non-name word in front
of the first name. Although it would appear to run in linear time, it
actually runs in constant time, \(T_4\), since we only allow a maximum
of five names to appear in succession.

Then, the full name is properly formatted, which depends on whether or
not any middlenames were found.

#+begin_src python
            if middle_names:
                full_name = "%s %s %s" % (
                    first_name, " ".join(middle_names), last_name
                )
            else:
                full_name = "%s %s" % (first_name, last_name)
#+end_src

This computation runs in constant time, \(T_5\).

Subsequently, it check to see if the full name is part of the
blacklisted names.

#+begin_src python
            full_name_up = full_name.upper()
            # Check if name is blacklisted.
            # The name is blacklisted if there exists a string in the
            # blacklist which is contained as a substring of the name.
            is_blacklisted = any(b in full_name_up for b in self._blacklist)
            # Name match is always high probability
            # and occurs only when first and last name are in the name lists
            # Set probability according to how many of the names were found
            # in the names lists
            if (first_match and last_match) or is_blacklisted:
                probability = 1.0
            elif first_match or last_match or middle_match:
                probability = 0.5
            else:
                continue
#+end_src

All of these run in constant time, \(T_6\), under the assumption that
the =self._blacklist= remain constant throughout the execution of the
programme.

Had the =expansive=-option been enabled, some additional computations
would have taken place:

#+begin_src python

            # If we have to do a second pass, cut this matched name out to
            # avoid duplicates
            if self._expansive:
                unmatched_text = unmatched_text.replace(matched_text, "", 1)
#+end_src

However, in the default algorithm we assume that the =expansive=-option
is disabled. Finally, a formatted match with the detected name along
with additional information is returned as a =dict= using the
=yield=-keyword:

#+begin_src python
            yield {
                "match": matched_text,
                "probability": probability,

                **make_context(match, text),

                "sensitivity": (
                    self.sensitivity.value if self.sensitivity else None
                ),
            }
#+end_src

This takes constant time, \(T_7\).

To summarize, the overall worst-case running time, \(T(n,m)\) of the
=match()=-function for the =NameRule= can be expressed as:

\begin{equation*}
T(n, m) = T_1 + T_2\cdot n + m(T_3 + T_4 + T_5 + T_6 + T_7)
\end{equation*}

Suppose there exists \(k \geq max(\{T_1,...,T_7\})\), then

\begin{align*}
k +  kn + &m(k + k + k + k + k) \geq \\
T_1 + T_2 \cdot n + &m(T_3 + T_4 + T_5 + T_6 + T_7) \\
&\Downarrow \\
k + kn + 5k&m \geq \\
T_1 + T_2 \cdot n + &m(T_3 + T_4 + T_5 + T_6 + T_7) \\
&\Downarrow \\
T(n, m) = k + kn + 5k&m = \mathcal{O}(n + m)
\end{align*}

And thus we have proved that the Asymptotic Upper-bound for the default
version of the matching algorithm of the =NameRule= is
\(\mathcal{O}(n + m)\).

A few other variables do appear in the algorithm: the number of elements
in the =blacklist= and =whitelist= datasets, \(p\), and the number of
recognized names \(q\) (loaded datasets). Assuming that \(p\) and \(q\)
are independent of \(n\) and \(m\) and that it does not change while the
algorithm is running, one can consider \(p\) and \(q\) to be constants
and thus still conclude that the =match()=-function for =NameRule= has
worst-case running time of \(\mathcal{O}(n + m)\).

*** The =expansive= option
:PROPERTIES:
:CUSTOM_ID: the-expansive-option
:END:
The only difference between the default version and enabling the
=expansive=-option is the last ~40 lines of code. The purpose of this
code is to detect occurances of stand-alone names that do not match the
=full_name_regex=, which requires at least a first name and a last name
to appear in succession.

#+begin_src python
...
            # If we have to do a second pass, cut this matched name out to
            # avoid duplicates
            if self._expansive:
                unmatched_text = unmatched_text.replace(matched_text, "", 1)

            yield {
                "match": matched_text,
                "probability": probability,

                **make_context(match, text),

                "sensitivity": (
                    self.sensitivity.value if self.sensitivity else None
                ),
            }
        if self._expansive:
            # Full name match done. Now check if there's any standalone names
            # in the remaining, i.e. so far unmatched string.
            name_regex = regex.compile(_name)
            it = name_regex.finditer(unmatched_text, overlapped=False)
            for m in it:
                matched = m.group(0)
                if is_name_component(
                        matched.upper(), self.first_names, self.last_names):
                    yield {
                        "match": matched,
                        "probability": 0.1,

                        # XXX: are the offsets here useful? (unmatched_text is
                        # something we've produced internally...)
                        **make_context(m, unmatched_text),

                        "sensitivity": (
                            self.sensitivity.value
                            if self.sensitivity else None
                        ),
                    }

...
#+end_src

First, any text that had previously been matched by the default
algorithm is removed using the =replace()=-method on a copy of the
entire text =unmatched_text=:

#+begin_src python
            # If we have to do a second pass, cut this matched name out to
            # avoid duplicates
            if self._expansive:
                unmatched_text = unmatched_text.replace(matched_text, "", 1)
#+end_src

This runs in linear time as all of the original text has to be altered,
and it runs \(m\) times. After that, the =name_regex= is compiled and
used to find stand-alone names using the =find_iter()=-method:

#+begin_src python
            name_regex = regex.compile(_name)
            it = name_regex.finditer(unmatched_text, overlapped=False)
#+end_src

For each stand-alone name found, as that 'name' could be anything that
is capital cased, it is checked that it is actually a recognized name
using the =is_name_component()=-function. If it is a recognized name, a
result is yielded:

#+begin_src python
                if is_name_component(
                        matched.upper(), self.first_names, self.last_names):
                    yield {
                        "match": matched,
                        "probability": 0.1,

                        # XXX: are the offsets here useful? (unmatched_text is
                        # something we've produced internally...)
                        **make_context(m, unmatched_text),

                        "sensitivity": (
                            self.sensitivity.value
                            if self.sensitivity else None
                        ),
                    }
#+end_src

Enabling the =expansive=-option does incur additional computation. It
does change the class of the Asymptotic upper-bound of the =NameRule=
matching algorithm. In the default version, it had the complexity class
\(\mathcal{O}(n + m)\). This changes for the worse to the class of
\(\mathcal{O}(n + nm)\), because of the replacement of the
=unmatched_text= varible.

** Address Rule
:PROPERTIES:
:CUSTOM_ID: address-rule
:END:
Similar in concept to the Name Rule, the Address Rule is designed to
detect street addresses in Denmark. Likewise, it is also encapsulated by
a class. This one is appropriately called =AddressRule=. Unlike the two
previous rules, the Address Rule doesn't have any options that can be
enabled, so there is only one algorithm to analyze.

On figure [[Figur 4]] below, there is a UML activity diagram that shows the overall
flow of the algorithm behind the =AddressRule=:

#+caption: Activity diagram for AddressRule algorithm
#+NAME: Figur 4
#+ATTR_LATEX: :width 6cm :height 10cm
[[file:address_activity.png]]

All of the important computations take place in the =match()= method:

#+begin_src python
    def match(self, text):
        self._load_datasets()

        def is_address_fragment(fragment, candidates):
            fragment = fragment.upper()
            return (fragment in candidates
                    and fragment not in self._whitelist)

        # First, check for full address
        addresses = match_full_address(text)
        for address in addresses:
            street_name = address[0]
            house_number = address[1] if address[1] else ''

            # Store the original matching text
            matched_text = address[5]

            street_match = is_address_fragment(street_name, self.street_names)
            street_name_up = street_name.upper()
            is_blacklisted = any([street_name_up in b for b in self._blacklist])
            if (street_match and house_number) or is_blacklisted:
                sensitivity = Sensitivity.CRITICAL
            elif street_match:
                sensitivity = Sensitivity.PROBLEM
            else:
                continue

            yield {
                "match": matched_text,
                "sensitivity": sensitivity.value
            }
#+end_src

Similar to the NameRule analyzed in the previous section, collections of
data sets are loaded and cached with the =self._load_datasets()= method:

#+begin_src python
    def _load_datasets(self):
        if self.street_names is None:
            # Convert list of str to upper case and to sets for efficient
            # lookup
            self.street_names = set(map(str.upper,
                                        common_loader.load_dataset(
                                            "addresses", "da_addresses")))
#+end_src

Under the assumption that the list of addresses are immutable, the call
to =self._load_datasets()= runs in constant time, \(T_1\). Since this is
cached, this computation only occures once on the first invokation of
=match()=.

Next, a locally-scoped function, called =is_address_fragment()=, is
defined:

#+begin_src python
        def is_address_fragment(fragment, candidates):
            fragment = fragment.upper()
            return (fragment in candidates
                    and fragment not in self._whitelist)
#+end_src

The call =fragment.upper()= takes linear time depending on the length of
the =fragment= parameter. Also, the membership test
=fragment in candidates and fragment not in self._whitelist= runs in
constant time, because the built-in =set()=-type in =python= uses
hashing to quickly insert and retrieve members. Overall, we find that
the =is_address_fragment()=-function has linear running time.

After that, all potential addresses are found using the
=match_full_address()=-function, which has the definition:

#+begin_src python
def match_full_address(text):
    """Return possible address matches in the given text as a `set`"""

    def strip_or_empty(string):
        return string.strip() if string else ""

    matches = set()
    it = full_address_regex.finditer(text, overlapped=False)
    for m in it:
        street_address = strip_or_empty(m.group("street_name"))
        house_number = strip_or_empty(m.group("house_number"))
        floor = strip_or_empty(m.group("floor"))
        zip_code = strip_or_empty(m.group("zip_code"))
        city = strip_or_empty(m.group("city"))

        matched_text = m.group(0)
        matches.add(
            (street_address, house_number, floor, zip_code, city, matched_text)
        )
    return matches
#+end_src

This function uses a pre-compiled regular expression,
=full_address_regex=, which findes substrings with the format:

#+begin_src python
# Match whitespace except newlines
_whitespace = r"[^\S\n\r]+"
_optional_comma = r",?"
_optional_whitespace = r"[^\S\n\r]?"


# match a number prepended to a street name, like '10.' or '10-'
_prepend_number = r"\d*[\.-]"
_street_name = r"\p{Lu}\p{L}*[\.\/-]?"
_simple_name = r"\p{Lu}\p{L}*[\.]?"
_house_number = r"[1-9][0-9]*[a-zA-Z]?"
_floor = r"[0-9]{{0,3}}\.?{0}(?:tv|th|mf|sal|[0-9]*)".format(_whitespace)
_zip_code = r"[1-9][0-9][0-9][0-9]"
_street_address = (
    r"(?P<street_name>(?:{0}{ws})?(?:{1}{ws})+)". format(
        _prepend_number,
        _street_name,
        ws=_optional_whitespace) +
    r"(?P<house_number>{0})?".format(_house_number))
_floor_number = r"(?P<floor>{0}{1})?".format(_whitespace, _floor)
_zip_city = r"(?P<zip_code>{0}){1}(?P<city>(?:{2}{3})+)".format(
    _zip_code,
    _whitespace,
    _simple_name,
    _optional_whitespace
)
full_address_regex = regex.compile(  # noqa: ECE001
    r"\b" + _street_address + _optional_comma + _floor_number + _optional_comma +
    r"(" + _optional_whitespace + _zip_city + r")?" + r"\b",
    regex.UNICODE
)
#+end_src

Every substring found using the =full_address_regex= is then for
stripped for excessive whitespace and returned as a =set()= of =tuple()=
with six elements:

#+begin_src python
    for m in it:
        street_address = strip_or_empty(m.group("street_name"))
        house_number = strip_or_empty(m.group("house_number"))
        floor = strip_or_empty(m.group("floor"))
        zip_code = strip_or_empty(m.group("zip_code"))
        city = strip_or_empty(m.group("city"))

        matched_text = m.group(0)
        matches.add(
            (street_address, house_number, floor, zip_code, city, matched_text)
        )
    return matches
#+end_src

The call to =match_full_address()= is run on the entire text, so it runs
in linear time, \(T_2n\). Next, each matched address found is checked to
ensure that it contains a street name, as a minimum, and a house number.

#+begin_src python
        for address in addresses:
            street_name = address[0]
            house_number = address[1] if address[1] else ''

            # Store the original matching text
            matched_text = address[5]

            street_match = is_address_fragment(street_name, self.street_names)
            street_name_up = street_name.upper()
            is_blacklisted = any([street_name_up in b for b in self._blacklist])
            if (street_match and house_number) or is_blacklisted:
                sensitivity = Sensitivity.CRITICAL
            elif street_match:
                sensitivity = Sensitivity.PROBLEM
            else:
                continue

            yield {
                "match": matched_text,
                "sensitivity": sensitivity.value
            }
#+end_src

If the match contains both street name and house number, or the street
name appears in the blacklist, the sensitivity of the match is deemed
=CRITICAL=. On the other hand, if only a non-blacklisted street name is
found then the sensitivity is deemed as =PROBLEM= instead. In both of
these cases, a result =dict()= is yielded with the matched text and the
sensitivity. Based on the assumptions that both the list of recognized
addresses and the blacklisted street names are invariants, all of the
computations in the code snippet above can be run in constant time,
\(T_3\), for each match, \(m\). In total, this is \(T_3m\) computations.

To summarize, the overall worst-case running time, \(T(n,m)\) of the
=match()=-function for the =AddressRule= can be expressed as:

\begin{equation*}
T(n, m) = T_1 + T_2\cdot n + T_3\cdot m
\end{equation*}

Suppose there exists \(k \geq max(\{T_1, T_2, T_3\})\), then

\begin{align*}
k +  kn + km &\geq T_1 + T_2 \cdot n + T_3\cdot m \\
&\Downarrow \\
T(n, m) = k + kn + k&m = \mathcal{O}(n + m)
\end{align*}

And thus we have proved that the Asymptotic Upper-bound for the default
version of the matching algorithm of the =AddressRule= is
\(\mathcal{O}(n + m)\). As with the =NameRule=, this conclusion rests on
the assumptions that the list of recognized addresses and the
blacklisted street names are constant and do not change.

** OrderedWordlist Rule
:PROPERTIES:
:CUSTOM_ID: orderedwordlist-rule
:END:
On figure [[Figur 5]] below, there is a UML activity diagram that shows the overall
flow of the algorithm behind the =OrderedWordlistRule=:

#+caption: Activity diagram for OrderedWordListRule algorithm
#+NAME: Figur 5
#+ATTR_LATEX: :width 6cm :height 10cm
[[file:wordlist_activity.png]]

The implemenation of the =match()=-method for =OrderedWordlistRule= is
rather straight forward.

#+begin_src python
    def match(self, content: str) -> Optional[Iterator[dict]]:
        if content is None:
            return
        folded_content = content.casefold()

        for wordlist in self._wordlists:
            yield from _match_wordlist(wordlist, content, folded_content)
#+end_src

First, it is asserted that the =content= (search text) is not empty.
This runs in constant time, \(T_1\). Then, the each character in
=content= is lower-cased using the =casefold()=-method, which runs in
linear time, \(T_2n\), where \(n\) is the length of the search text,
=content=. A wordlist may contain nested wordlists. The search text,
=content=, is then matched against each nested wordlist using the
=_match_wordlist()=-function. The =_match_wordlist()=-function has the
following definition:

#+begin_src python
def _match_wordlist(wordlist, content, folded_content):
    slices = []
    start_at = 0
    end_at = len(folded_content)
    for word in wordlist:
        try:
            index = folded_content[start_at:end_at].index(word.casefold())
            true_index = start_at + index
            start_at = true_index + len(word)
            end_at = start_at + 32
            slices.append(slice(true_index, start_at))
        except ValueError:
            return

    starts_at = slices[0].start
    yield {
        "match": " ".join(wordlist),
        "offset": slices[0].start,
        "context": content[
                slice(max(starts_at - 50, 0), slices[-1].stop + 50)],
        "context_offset": min(starts_at, 50)
    }
#+end_src

It is attempted to find each =word= in the nested wordlist in sequence.
This is done by searching the entire =content= for the first word,
initially, and then search the next 32 character for the subsequent
word. The =.index()=-method is used to search for the =word= in the
=folded_content=. It returns the index of the occurance of the first
character of =word= in =folded_content= upon success. If the =word= is
not found, a =ValueError= exception is thrown and the search stops. If
all the words in =wordlists= are found in succession in the
=folded_content=, a match result is returned as a =dict()= with the
matched words and the surrounding context among other things.

What about the complexity of this =_match_wordlist()=-function? For each
=wordlist=, the entire =content= might be searched if there are enough
words in the =wordlist= and they are distributed within 32 characters of
each other in sequence. This takes linear time, \(T_3n\), where \(n\) is
the length of =content=. Also, for every search of a word, a substring
of =folded_content[start_at:end_at]= is copied. Since, =start_at = 0=
and =end_at = len(folded_content)= for the first word, then
=folded_content[start_at:end_at]= is the same as =folded_content=. That
is, a full copy of =folded_content= is made, which has the same length
as =content=, \(n\), which runs in linear time, \(T_4n\). The total
running time of =_match_wordlist()= is \((T_3 + T_4)n\).

However, the =_match_wordlist()=-function is called on every nested
wordlist in =self._wordlists=. Let \(w\) denote the number of nested
wordlists in =self._wordlists=.

To summarize, the overall worst-case running time, \(T(n,w)\) of the
=match()=-function for the =OrderedWordlistRule= can be expressed as:

\begin{equation*}
T(n, w) = T_1 + T_2\cdot n + (T_3 + T_4)nw
\end{equation*}

Suppose there exists \(k \geq max(\{T_1, T_2, T_3, T_4\})\), then

\begin{align*}
k +  kn + 2knw &\geq T_1 + T_2 \cdot n + (T_3 + T_4)nw \\
&\Downarrow \\
T(n, w) = k + kn + 2k&nw = \mathcal{O}(n + nw)
\end{align*}

And thus we have proved that the Asymptotic Upper-bound for the default
version of the matching algorithm of the =OrderedWordlistRule= is
\(\mathcal{O}(n + nw)\). Again, this conclusion rests on the assumption
that the list of nested wordlists, =self._wordlists=, is invariant.
